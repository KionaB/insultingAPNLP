{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74cf0be-4822-40bb-a192-cf3c965f41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b311ac0-c116-406c-a232-9973086c8fad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-12-09 22:20:23.910683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-09 22:20:24.062106: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import itertools\n",
    "# from tabulate import tabulate\n",
    "# import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69114aa4-326f-4f7c-8280-5a5913b872eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fasttext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 194\u001b[0m\n\u001b[1;32m      2\u001b[0m animals \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAardvark\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlligator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZebra\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m ]\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# You need to download the model file first — e.g. `cc.en.300.bin.gz`\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Download from fastText website: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Then load it:\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc.en.300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This function does the projection, changed from the previous one to one I understand intuitively\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproj_meas\u001b[39m(v1, v2, v3):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fasttext' is not defined"
     ]
    }
   ],
   "source": [
    "# Source: https://englishteabreak.netlify.app/hub/animals\n",
    "animals = [\n",
    "    \"Aardvark\",\n",
    "    \"Alligator\",\n",
    "    \"Alpaca\",\n",
    "    \"Ant\",\n",
    "    \"Anteater\",\n",
    "    \"Antelope\",\n",
    "    \"Armadillo\",\n",
    "    \"Baboon\",\n",
    "    \"Badger\",\n",
    "    \"Barracuda\",\n",
    "    \"Bat\",\n",
    "    \"Bear\",\n",
    "    \"Beaver\",\n",
    "    \"Bee\",\n",
    "    \"Beetle\",\n",
    "    \"Bird\",\n",
    "    \"Bison\",\n",
    "    \"Bobcat\",\n",
    "    \"Buffalo\",\n",
    "    \"Bull\",\n",
    "    \"Butterfly\",\n",
    "    \"Calf\",\n",
    "    \"Camel\",\n",
    "    \"Capybara\",\n",
    "    \"Caribou\",\n",
    "    \"Cat\",\n",
    "    \"Caterpillar\",\n",
    "    \"Centipede\",\n",
    "    \"Chameleon\",\n",
    "    \"Cheetah\",\n",
    "    \"Chick\",\n",
    "    \"Chicken\",\n",
    "    \"Chimpanzee\",\n",
    "    \"Chinchilla\",\n",
    "    \"Chipmunk\",\n",
    "    \"Cicada\",\n",
    "    \"Clam\",\n",
    "    \"Cockroach\",\n",
    "    # \"Cougar\", This can be a bad slur and is therefore removed\n",
    "    \"Cow\",\n",
    "    \"Crab\",\n",
    "    \"Crane\",\n",
    "    \"Crayfish\",\n",
    "    \"Cricket\",\n",
    "    \"Crocodile\",\n",
    "    \"Crow\",\n",
    "    \"Cub\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Dolphin\",\n",
    "    \"Donkey\",\n",
    "    \"Dove\",\n",
    "    \"Dragonfly\",\n",
    "    \"Duck\",\n",
    "    \"Duckling\",\n",
    "    \"Eagle\",\n",
    "    \"Elephant\",\n",
    "    \"Elk\",\n",
    "    \"Fawn\",\n",
    "    \"Ferret\",\n",
    "    \"Firefly\",\n",
    "    \"Fish\",\n",
    "    \"Flamingo\",\n",
    "    \"Flea\",\n",
    "    \"Fly\",\n",
    "    \"Foal\",\n",
    "    \"Fox\",\n",
    "    \"Frog\",\n",
    "    \"Gazelle\",\n",
    "    \"Gecko\",\n",
    "    \"Giraffe\",\n",
    "    \"Goat\",\n",
    "    \"Goose\",\n",
    "    \"Gopher\",\n",
    "    \"Gorilla\",\n",
    "    \"Grasshopper\",\n",
    "    \"Guinea Pig\",\n",
    "    \"Hamster\",\n",
    "    \"Hawk\",\n",
    "    \"Hedgehog\",\n",
    "    \"Hippo\",\n",
    "    \"Horse\",\n",
    "    \"Hummingbird\",\n",
    "    \"Hyena\",\n",
    "    \"Iguana\",\n",
    "    \"Jaguar\",\n",
    "    \"Jellyfish\",\n",
    "    \"Kangaroo\",\n",
    "    \"Kitten\",\n",
    "    \"Koala\",\n",
    "    \"Ladybug\",\n",
    "    \"Lamb\",\n",
    "    \"Lemming\",\n",
    "    \"Lemur\",\n",
    "    \"Leopard\",\n",
    "    \"Lion\",\n",
    "    \"Lizard\",\n",
    "    \"Llama\",\n",
    "    \"Lobster\",\n",
    "    \"Lynx\",\n",
    "    \"Manatee\",\n",
    "    \"Manta Ray\",\n",
    "    \"Meerkat\",\n",
    "    \"Monkey\",\n",
    "    \"Moose\",\n",
    "    \"Mosquito\",\n",
    "    \"Moth\",\n",
    "    \"Mountain Lion\",\n",
    "    \"Mouse\",\n",
    "    \"Mule\",\n",
    "    \"Muskrat\",\n",
    "    \"Mussel\",\n",
    "    \"Narwhal\",\n",
    "    \"Newt\",\n",
    "    \"Ocelot\",\n",
    "    \"Octopus\",\n",
    "    \"Opossum\",\n",
    "    \"Orangutan\",\n",
    "    \"Otter\",\n",
    "    \"Owl\",\n",
    "    \"Oyster\",\n",
    "    \"Panda\",\n",
    "    \"Parrot\",\n",
    "    \"Pelican\",\n",
    "    \"Penguin\",\n",
    "    \"Pig\",\n",
    "    \"Pigeon\",\n",
    "    \"Piglet\",\n",
    "    \"Piranha\",\n",
    "    \"Platypus\",\n",
    "    \"Pony\",\n",
    "    \"Porcupine\",\n",
    "    \"Prairie Dog\",\n",
    "    \"Prawn\",\n",
    "    \"Praying Mantis\",\n",
    "    \"Puma\",\n",
    "    \"Puppy\",\n",
    "    \"Quail\",\n",
    "    \"Rabbit\",\n",
    "    \"Raccoon\",\n",
    "    \"Rat\",\n",
    "    \"Rhino\",\n",
    "    \"Robin\",\n",
    "    \"Rooster\",\n",
    "    \"Salamander\",\n",
    "    \"Scorpion\",\n",
    "    \"Sea Lion\",\n",
    "    \"Sea Urchin\",\n",
    "    \"Seagull\",\n",
    "    \"Seal\",\n",
    "    \"Shark\",\n",
    "    \"Sheep\",\n",
    "    \"Shrew\",\n",
    "    \"Shrimp\",\n",
    "    \"Silverfish\",\n",
    "    \"Skunk\",\n",
    "    \"Sloth\",\n",
    "    \"Slug\",\n",
    "    \"Snail\",\n",
    "    \"Snake\",\n",
    "    \"Sparrow\",\n",
    "    \"Spider\",\n",
    "    \"Squid\",\n",
    "    \"Squirrel\",\n",
    "    \"Starfish\",\n",
    "    \"Stork\",\n",
    "    \"Swan\",\n",
    "    \"Tadpole\",\n",
    "    \"Termite\",\n",
    "    \"Tick\",\n",
    "    \"Tiger\",\n",
    "    \"Toad\",\n",
    "    \"Tortoise\",\n",
    "    \"Turkey\",\n",
    "    \"Turtle\",\n",
    "    \"Wallaby\",\n",
    "    \"Walrus\",\n",
    "    \"Weasel\",\n",
    "    \"Whale\",\n",
    "    \"Wildebeest\",\n",
    "    \"Wolf\",\n",
    "    \"Wombat\",\n",
    "    \"Woodpecker\",\n",
    "    \"Worm\",\n",
    "    \"Yak\",\n",
    "    \"Zebra\"\n",
    "]\n",
    "\n",
    "# You need to download the model file first — e.g. `cc.en.300.bin.gz`\n",
    "# Download from fastText website: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# Then load it:\n",
    "model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# This function does the projection, changed from the previous one to one I understand intuitively\n",
    "def proj_meas(v1, v2, v3):\n",
    "    dl = np.linalg.norm(v1 - v3)\n",
    "    dr = np.linalg.norm(v2 - v3)\n",
    "    dh = np.linalg.norm(v1 - v2)\n",
    "    \n",
    "    if dl != 0:\n",
    "        alpha = np.arccos((dl**2+dh**2-dr**2)/(2*dl*dh))\n",
    "    else:\n",
    "        alpha = 0\n",
    "    proj, d = dl*np.cos(alpha), dl*np.sin(alpha)\n",
    "    t = proj/dh\n",
    "    # print('alpha, ', alpha, 'should angle be >pi/2', dr>dh)\n",
    "    return d, t\n",
    "\n",
    "def find_antonym(word, num_neighbours=300):\n",
    "    neighbours = [w for _, w in model.get_nearest_neighbors(word, num_neighbours)]\n",
    "\n",
    "    for nb in neighbours:\n",
    "        for syn in wn.synsets(nb):\n",
    "            for lemma in syn.lemmas():\n",
    "                for ant in lemma.antonyms():\n",
    "                    return ant.name()  # <-- get string, not Lemma object\n",
    "\n",
    "    return None  # no antonym found\n",
    "\n",
    "def create_regular_list(word, word_list, num_neighbours=300):\n",
    "    bad_words, good_words = [word], [find_antonym(word, num_neighbours)]\n",
    "\n",
    "    bad_vecs = np.array([model.get_word_vector(w) for w in bad_words])\n",
    "    good_vecs = np.array([model.get_word_vector(w) for w in good_words])\n",
    "\n",
    "    bad_mean = np.mean(bad_vecs, axis=0)\n",
    "    good_mean = np.mean(good_vecs, axis=0)\n",
    "\n",
    "    scale_scores = []\n",
    "    dist_scores = []\n",
    "\n",
    "    # Compute t and distance for each word\n",
    "    for word in word_list:\n",
    "        deter = model.get_word_vector(word)\n",
    "        d, t = proj_meas(bad_mean, good_mean, deter)\n",
    "        scale_scores.append(t)\n",
    "        dist_scores.append(d)\n",
    "\n",
    "    scores = np.array(scale_scores)\n",
    "    dists = np.array(dist_scores)\n",
    "    words = np.array(word_list)\n",
    "    \n",
    "    ordered_words = list(words[np.argsort(scores)])\n",
    "    ordered_scores = list(scores[np.argsort(scores)])\n",
    "    return ordered_words, ordered_scores, bad_words, good_words\n",
    "\n",
    "def create_t_adjusted_list(word, word_list, num_neighbours=100, weight=1):\n",
    "    bad_words, good_words = [word], [find_antonym(word, num_neighbours)]\n",
    "    bad_vecs = np.array([model.get_word_vector(w) for w in bad_words])\n",
    "    good_vecs = np.array([model.get_word_vector(w) for w in good_words])\n",
    "\n",
    "    bad_mean = np.mean(bad_vecs, axis=0)\n",
    "    good_mean = np.mean(good_vecs, axis=0)\n",
    "\n",
    "    scale_scores = []\n",
    "    dist_scores = []\n",
    "\n",
    "    # Compute t and distance for each word\n",
    "    for word in word_list:\n",
    "        deter = model.get_word_vector(word)\n",
    "        d, t = proj_meas(bad_mean, good_mean, deter)\n",
    "        scale_scores.append(t)\n",
    "        dist_scores.append(d)\n",
    "\n",
    "    scores = np.array(scale_scores)\n",
    "    dists = np.array(dist_scores)\n",
    "    words = np.array(word_list)\n",
    "    \n",
    "    normed_dists = (dists - np.min(dists)) / (np.max(dists) - np.min(dists))\n",
    "    t_avg = np.mean(scores)\n",
    "    t_adjusted = scores + weight * (t_avg - scores) * normed_dists\n",
    "\n",
    "    ordered_scores = list(t_adjusted[np.argsort(t_adjusted)])\n",
    "    ordered_words = list(words[np.argsort(t_adjusted)])\n",
    "    # ordered_scores = list(scores[np.argsort(scores)])\n",
    "    return ordered_words, ordered_scores, bad_words, good_words\n",
    "\n",
    "\n",
    "word = 'ugly'\n",
    "# ordered_words, ordered_scores, bad_words, good_words = create_regular_list(word, animals)\n",
    "ordered_words, ordered_scores, bad_words, good_words = create_t_adjusted_list(word, animals)\n",
    "print(bad_words)\n",
    "print('from', word, ' to ', good_words[0])\n",
    "for word2, score in zip(ordered_words, ordered_scores):\n",
    "    print(word2, score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
